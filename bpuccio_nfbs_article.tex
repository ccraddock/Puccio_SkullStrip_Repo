

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocol]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
%\usepackage{notes}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\def\includegraphic{}
%\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{DATA NOTE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{The Preprocessed Connectomes Project repository of manually-corrected skull-stripped T1-weighted anatomical MRI data.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={bpuccio@nki.rfmh.org}   % email address
]{\inits{BP}\fnm{Benjamin} \snm{Puccio}}
\author[addressref={aff2}]{\inits{JPP}\fnm{James P} \snm{Pooley}}
\author[addressref={aff2}]{\inits{JSP}\fnm{John S} \snm{Pellman}}
\author[addressref={aff1}]{\inits{ECT}\fnm{Elise C} \snm{Taverna}}
\author[
   addressref={aff1,aff2},
   email={ccraddock@nki.rfmh.org},
   corref={aff1,aff2}                       % id of corresponding address, if any
]{\inits{RCC}\fnm{R Cameron} \snm{Craddock}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{Computational Neuroimaging Lab, Center for Biomedical Imaging and Neuromodulation, Nathan Kline Institute for Psychiatric Research}, % university, etc
  \street{140 Old Orangeburg Rd},                     %
  \postcode{10962},                                % post or zip code
  \city{Orangeburg, NY},                              % city
  \cny{USA}                                    % country
}
\address[id=aff2]{%
  \orgname{Center for the Developing Brain, Child Mind Institute},
  \street{445 Park Ave},
  \postcode{10022},
  \city{New York, NY},
  \cny{USA}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract

\parttitle{Background}
Skull-stripping is the procedure of removing non-brain tissue from anatomical MRI data. This procedure is necessary for calculating brain volume and for improving the quality of other image processing steps. Developing new skull-stripping algorithms and evaluating their performance requires \emph{gold standard} data from a variety of different scanners and acquisition methods. We complement existing repositories with manually-corrected brain masks for 125 T1-weighted anatomical scans from the Nathan Kline Institute Enhanced Rockland Sample Neurofeedback Study.

\parttitle{Findings}
Skull-stripped images were obtained using a semi-automated procedure that involved skull-stripping the data using the brain extraction based on non local segmentation technique (BEaST) software and manually correcting the worst results. Corrected brain masks were added into the BEaST library and the procedure was reiterated until acceptable brainmasks were available for all images. In total, 85 of the skull-stripped images were hand-edited and 40 were deemed to not need editing. The results are brain masks for the 125 images along with a BEaST library for automatically skull-stripping other data.


\parttitle{Conclusion}
Skull-stripped anatomical images from the Neurofeedback sample are available for download from the Preprocessed Connectomes Project. This data provides researchers with gold standard training and testing data for developing new skull-stripping algorithms and for evaluating their impact on other aspects of MRI preprocessing. We have illustrated the utility of these data as a reference for comparing various automatic methods and evaluated the performance of the newly created library on independent data.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{brain extraction}
\kwd{skull-stripping}
\kwd{data sharing}
\kwd{brain mask}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>


%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Data Description}

One of the many challenges facing the analysis of magnetic resonance imaging (MRI) data is achieving accurate brain extraction from the data. Brain extraction, also known as skull-stripping, aims to remove all non-brain tissue from an image. This is one of the preliminary steps in preprocessing and the quality of its result affects the subsequent steps, such as image registration and brain matter segmentation. There are a multitude of challenges that surround the process of brain extraction. The manual creation and correction of brain masks is tedious, time-consuming, and susceptible to experimenter bias. On the other hand, fully automated brain extraction is not a simple image segmentation problem. Brains in images can differ in orientation and morphology, especially in pediatric, geriatric, and pathological brains. In addition, non-brain tissue may resemble brain in terms of voxel intensity. Differences in MRI scanner, acquisition sequence, and scan parameters can also have an effect on automated algorithms due to differences in image contrast, quality, and orientation. Image segmentation techniques with low computational time, high accuracy, and high flexibility are extremely desirable. 

Developing new automated skull-stripping methods, and comparing these with existing methods, requires large quantities of \emph{gold standard} skull-stripped data acquired from a variety of scanners using a variety of sequences and parameters. This is due to the variation in performance of algorithms using different MRI data. Repositories containing gold standard skull-stripped data already exist: The Alzheimer's Disease Neuroimaging Inititative (ADNI) \cite{pmid17476317}; BrainWeb: Simulated Brain Database (SBD) \cite{brainweb}; The Internet Brain Segmentation Repository (IBSR) at the Center for Morphometric Analysis \cite{IBSR}; the LONI Probabilistic Brain Atlas (LPBA40) at the UCLA Laboratory of Neuro Imaging \cite{lpba40}; and The Open Access Series of Imaging Studies (OASIS) \cite{oasis}, the last of which is not manually delineated but has been used as gold standard data \cite{Iglesias2011, MASS}. We extend and complement these existing repositories by releasing manually corrected skull strips for 125 individuals from the NKI Enhanced Rockland Sample Neurofeedback study.  

\subsection*{Data acquisition}

The repository was constructed from defaced and anonymized anatomical data downloaded from the Nathan Kline Institute Enhanced Rockland Sample Neurofeedback Study (NFB) \cite{Rockland}. The NFB is a 3-visit study that involves a deep phenotypic assessment on the first and second visits \cite{Nooner}, a 1-hour connectomic MRI scan on the second visit, and a 1-hour neurofeedback scan on the last visit. Up to 3 months may have passed between the first and last visits. The 125 participants included 77 females and 48 males in the 21 - 45 age range (average: 31, standard deviation: 6.6). Sixty-six (66) of the participants had one or more current or past psychiatric diagnosis as determined by the structured clinical interview for the DSM IV (SCID) \cite{skid} [see Table \ref{psych}]. No brain abnormalities or incidental findings were present in the included images as determined by a board-certified neuroradiologist. None of the participants had any other major medical condition such as cancer or AIDS. All experimental procedures were performed with institutional review board approval and only after informed consent was obtained.

\begin{table}[h!]
\caption{Neurofeedback Participant Diagnoses}
      \begin{tabular}{ p{6.0cm}|p{0.8cm} }
        Diagnosis (SCID \#) & \# \\ \hline
        No Diagnosis or Condition on Axis I nan (V71.09) & 59  \\ \hline
        Alcohol Abuse Past (305) & 18  \\ \hline
        Major Depressive Disorder, Single Episode, In Full Remission nan (296.26) & 12  \\ \hline
        Cannabis Abuse Current (305.2) & 11  \\ \hline
        Cannabis Dependence Past (304.3) & 11  \\ \hline
        Major Depressive Disorder, Recurrent, In Full Remission Past (296.36) & 7  \\ \hline
        Major Depressive Disorder, Single Episode, Unspecified Past (296.2) & 5  \\ \hline
        Posttraumatic Stress Disorder Current (309.81) & 5  \\ \hline
        Alcohol Dependence Past (303.9) & 5  \\ \hline
        Specific Phobia Past (300.29) & 5  \\ \hline
        Generalized Anxiety Disorder Current (300.02) & 4  \\ \hline
        Attention-Deficit/Hyperactivity Disorder, Predominantly Inattentive Type Current (314) & 3  \\ \hline
        Attention-Deficit/Hyperactivity Disorder NOS Current (314.9) & 3  \\ \hline
        Attention-Deficit/Hyperactivity Disorder, Predominantly Hyperactive-Impulsive Type Current (314.01) & 3  \\ \hline
        Alcohol Abuse Past (305.0) & 3  \\ \hline
        Cocaine Abuse Past (305.6) & 2  \\ \hline
        Anorexia Nervosa Past (307.1) & 2  \\ \hline
        Anxiety Disorder NOS Current (300) & 2  \\ \hline
        Panic Disorder Without Agoraphobia Past (300.01) & 2  \\ \hline
        Social Phobia Current (300.23) & 2  \\ \hline
        Agoraphobia Without History of Panic Disorder Current (300.22) & 2  \\ \hline
        Panic Disorder With Agoraphobia Past (300.21) & 2  \\ \hline
        Hallucinogen Abuse Past (305.3) & 2  \\ \hline
        Cocaine Dependence Past (304.2) & 2  \\ \hline
        Trichotillomania nan (312.39) & 1  \\ \hline
        Attention-Deficit/Hyperactivity Disorder, Predominantly Inattentive Type Current (314.0) & 1  \\ \hline
        Bulimia Nervosa Current (307.51) & 1  \\ \hline
        Major Depressive Disorder, Recurrent, In Partial Remission Past (296.35) & 1  \\ \hline
        Major Depressive Disorder, Recurrent, Moderate Current (296.32) & 1  \\ \hline
        Bereavement nan (V62.82) & 1  \\ \hline
        Major Depressive Disorder, Recurrent, In Full Remission & 1  \\ \hline
        Alcohol Abuse, Current & 1  \\ \hline
        Hallucinogen Dependence Past (304.5) & 1  \\ \hline
        Obsessive-Compulsive Disorder Current (300.3) & 1  \\ \hline
        Body Dysmorphic Disorder Current (300.7) & 1  \\ \hline
        Eating Disorder NOS Past (307.5) & 1  \\ \hline
        Phencyclidine Abuse Past (305.9) & 1  \\ \hline
        Delusional Disorder Mixed Type (297.1) & 1  \\ \hline
        Amphetamine Dependence Past (304.4) & 1  \\ \hline
        Opioid Abuse Past (305.5) & 1  \\ \hline
        Sedative, Hypnotic, or Anxiolytic Dependence Past (304.1) & 1 \\ \hline
      \end{tabular}
      \label{psych}
\end{table}


Anatomical MRI data from the third visit of the NFB protcol was used to build the Neurofeedback Skull-stripped (NFBS) repository. MRI data were collected on a 3 T Siemens Magnetom TIM Trio scanner (Siemens Medical Solutions USA: Malvern PA, USA) using a 12-channel head coil. Anatomical images were acquired at 1 x 1 x 1 mm$^3$ resolution with a 3D T1-weighted magnetization-prepared rapid acquisition gradient-echo (MPRAGE) \cite{Mugler1990} sequence in 192 sagittal partitions each with a 256 x 256 mm$^2$ field of view (FOV), 2600 ms repetition time (TR) , 3.02 ms echo time (TE), 900 ms inversion time (TI), 8$^{\circ}$ flip angle (FA), and generalized auto-calibrating partially parallel acquisition (GRAPPA) accelerationc\cite{Griswold2002} factor of 2 with 32 reference lines. The anatomical data was acquired immediately after a fast localizer scan and proceeded the collection of a variety of other scans \cite{nki_mrproto}, whose description is beyond the scope of this report.

\subsection*{Brain Mask Definition}
Many researchers differ on the standard for what to include and exclude from the brain. Some brain extraction methods, such as brainwash, include the dura mater in the brain mask to use as a reference for measurements \cite{Brainwash}. The standard we used was adapted from Eskildsen et al (2012) \cite{Eskildsen2012}. Non-brain tissue is defined as skin, skull, eyes, dura mater, external blood vessels and nerves (e.g., optic chiasm, superior sagittal sinus, and transverse sinus). Cerebrum, cerebellum, brainstem, and internal vessels and arteries are included in the brain, along with cerebrospinal fluid (CSF) in ventricles, internal cisterns, and deep sulci. 


\subsection*{NFBS Repository Construction}
The BEaST method (brain extraction based on nonlocal segmentation technique) was used to initially skull-strip the 125 anatomical T1-weighted images \cite{Eskildsen2012}. This software uses a patch-based label fusion method that labels each voxel in the brain boundary volume by comparing it to similar locations in a library of segmented priors. The segmentation technique also incorporates a multi-resolution framework in order to reduce computational time. The version of BEaST used was 1.15.00 and our implementation was based off of a shell script written by Qingyang Li \cite{rpubs}. The standard parameters were used in the configuration files and beast-library-1.1 was used for the initial skull-strip of the data. Before running {\tt mincbeast}, the main segmentation script of BEaST, the anatomical images were normalized using the {\tt beast\_normalize} script. {\tt Mincbeast} was run using the probability filter setting, which smoothed the manual edits, and the fill setting, which filled any holes in the masks. We found the failure rate for masks using BEaST was similar to that of the published rate of approximately 29\% \cite{Eskildsen2012}. Visual inspection of these initial skull-stripped images indicated whether additional edits were necessary. 

Manual edits were performed using the Freeview visualization tool from the FreeSurfer software package \cite{freesurfer}. The anatomical image was loaded as a track volume and the brain mask was loaded as a volume. The voxel edit mode was then used to include or exclude voxels in the mask. As previously mentioned, all exterior non-brain tissue was removed from the head image, specifically the skull, scalp, fat, muscle, dura mater, and external blood vessels and nerves \ref{fig:edit}. Time spent editing each mask ranged from 1--8 hours, depending on the quality of the anatomical image and the BEaST mask. Afterwards, manually edited masks were used to populate the prior library of BEaST. This iterative bootstrapping technique was repeated until approximately 85 of the datasets were manually edited and all skull-strips were considered to be acceptable. 

\begin{figure}[h!]
    \includegraphics[width=0.4\textwidth]{edit.png}
    \caption{\csentence{Manual Editing}
Axial and coronal slices in AFNI viewer of the brain mask and image pair, before and after manual editing in Freeview. The anatomical MRI image was loaded into the viewer as a grayscale image. The mask, which can be seen in a transparent red, was loaded as an overlay image.}
    \label{fig:edit}
\end{figure}

For each of the 125 subjects, the repository contains the de-faced and anonymized anatomical T1-weighted image, skull-stripped brain image, and brain mask. Each of these are in compressed NIfTI file format (.nii.gz). The size of the entire data set is around 1.9 GB.


\subsection*{Data Validation}

The semi-automated skull-stripping procedure was repeated until all brain masks were determined to be acceptable by two raters (BP and ET). Once this was completed, the brain masks were used as gold standard data for comparing different automated skull-stripping algorithms.  Additionally, we evaluated the performance of the newly corrected BEaST library by comparing it to other skull-stripping methods on data from the Internet Brain Segmentation Repository (IBSR) \cite{IBSR} and LONI Probabilistic Brain Atlas (LPBA40) \cite{lpba40}.


\subsubsection*{Skull-Stripping Algorithms}
A wide variety of algorithms have been developed \cite{Iglesias2011, MASS, Brainwash, Sadananthan2010, Lutkenhoff2014, Wang2014, pmid21195780, bse}, but we focused on FSL's BET \cite{Smith2002}, AFNI's 3dSkullStrip \cite{afni}, and FreeSurfer's Hybrid Watershed Algorithm (HWA) \cite{Segonne2004} based on their popularity.

\begin{itemize}
    \item The \emph{Brain Extraction Technique (BET)} is an algorithm incorporated in the FSL software that is based on a deformable model of the surface of the brain \cite{Smith2002}. First, an intensity histogram is used to find the center of gravity of the head. Then a tessellated sphere is initialized around the center of gravity and expanded by locally adaptive forces. The method can also incorporate T2-weighted images to isolate the inner and outer skull and scalp. The bias field and neck setting ({\tt bet -B}) was used since the anatomical images contained the subjects' necks. The version of FSL that was used was 5.0.7.
    
    \item \emph{3dSkullStrip} is a modified version of BET that is incorporated in the AFNI toolkit \cite{afni}. The algorithm begins by preprocessing the image to correct for spatial variations in image intensity and repositioning the brain to roughly the center of the image. Then a modified algorithm based on BET is used to expand a mesh sphere until it envelops the entire brain surface. Among the modifications are procedures to avoid the eyes and ventricles and operations to avoid cutting into the brain. The version of the AFNI toolkit that was used was AFNI\_2011\_12\_21\_1014. 
    
    \item \emph{FreeSurfer's Hybrid Watershed Algorithm (HWA)} is a hybrid technique that uses a watershed algorithm in combination with a deformable surface algorithm \cite{Segonne2004}. The watershed algorithm is first used to create an initial mask under the assumption of the connectivity of white matter. Then a deformable surface model is used to incorporate geometric constraints into the mask. The version of FreeSurfer that was used was 5.3.0.
\end{itemize}

 

\subsubsection*{Data Analysis}

To illustrate the use of the NFBS as testing data, it was used to compare the performance of BET, 3dSkullStrip and HWA for automatically skull-stripping the original NFB data. In a second analysis we compared the performance of the NFBS BEaST library to the default BEaST library and the three aforementioned methods. Each of the methods were used to skull-strip data from the IBSR (version 2.0) and LPBA40 \cite{IBSR, lpba40}. To insure consistent image orientation across methods and datasets, they were all converted to LPI orientation using AFNI's \texttt{3dresample} program \cite{afni}. Additionally, a step function was applied to all of the outputs using AFNI's \texttt{3dcalc} tool to binarize all of the generated masks.

\par The performance of the various methods were compared using the Dice similarity \cite{Dice1945} between the mask generated for an image and its corresponding reference (`gold standard') mask. Dice was calculated using: $D = 2 \cdot | A \cap B | / (|A| + |B|)$, where $A$ is the set of voxels in the test mask, $B$ is the set of voxels in the gold standard data mask, $A \cap B$ is the intersection of $A$ and $B$, and $|\cdot|$ is the number of voxels in a set. Dice was implemented in custom Python scripts that used the NiBabel neuroimaging package \cite{NiBabel} for data input. Dice coefficients were subsequently graphed as box plots using the ggplot2 package \cite{ggplot} for the R statistical computing language \cite{R}.


\subsubsection*{Results}
Figure \ref{fig:boxplot_NFBS} displays box plots of the Dice coefficients that result from using NFBS as gold standard data. The results indicate that 3dSkullStrip performed significantly better of the three methods, with HWA coming in second. In particular, average Dice similarity coefficients were 0.893 $\pm$ 0.027 for BET, 0.949 $\pm$ 0.009 for 3dSkullStrip, and 0.900 $\pm$ 0.011 for HWA. It is perhaps worth noting that BET, the method that performed worst on the NFBS library, took substantially more time to run (25 min) compared to 3dSkullStrip (2 min) and HWA (1 min.). 

\begin{figure}[h!]
    \includegraphics[]{boxplot_nfbs.pdf}
    \caption{\csentence{Comparison of methods on NFBS}. Boxplots of Dice coefficients measuring the similarity between masks generated from each image using BET, 3dSkullStrip HWA and the image's corresponding reference brain masks. }
    \label{fig:boxplot_NFBS}
\end{figure}

Switching now from using NFBS as the repository of gold standard skullstripped images to using the IBSR and LPBA40 repositories as the source of gold standard images, Figure \ref{fig:boxplot_I&L} shows box plots of the Dice similarity coefficients for BET, 3dSkullStrip, HWA, BEaST using beast-library-1.1, and BEaST using NFBS as the library of priors. For IBSR, 3dSkullStrip performs better than BET and HWA, similarly to NFBS. However, for LPBA40, BET performs much better than the other two algorithms. The BEaST method was also applied to the anatomical data in these repositories using two different methods: first with the original beast-library-1.1 set as the prior library, and second with the entire NFBS set as the prior library. 


For the BEaST method, it can be said that using NFBS as the prior library resulted in higher average Dice similarity coefficients and smaller standard deviations\footnote{BEaST was unable to segment 1 subject, IBSR\_11, in IBSR, only when using beast-library-1.1. For LPBA40, BEaST was also unable to segment 1 subject, S35, when using beast-library-1.1 and NFBS. These subjects were left out of the Dice calculations.}. Differences in Dice coefficients between datasets may be due the size and quality of the NFB study, as well as the pathology and age of the participants. There also may be differences in the standard of the masks, such as length of brainstem and inclusion of exterior nerves and sinuses. 

\begin{figure}[ht!]
\includegraphics[]{boxplot_IL.pdf}
    \caption{\csentence{Dice Similarity Coefficients for IBSR \& LPBA40.}
Box plot of Dice coefficients for BET, 3dSkullStrip, HWA, BEaST using beast-library-1.1, and BEaST using NFBS as the library of priors. One subject was left out of the Dice calculation for each of the following: BEaST w/ beast-library-1.1 on IBSR (IBSR\_11), BEaST w/ beast-library-1.1 on LPBA40 (S35), and BEaST w/ NFBS on LPBA40 (S35).}
    \label{fig:boxplot_I&L}
\end{figure}

Placing our results in the context of other skullstripping comparisons, differences between the Dice coefficients reported here and values already published in the literature may be due to the version and implementation of the skull stripping algorithms, a possibility that has received support in the literature \cite{Iglesias2011}. These differences may also result from our application of AFNI's 3dcalc step function to the skull-stripped images in order to get a value determined more by brain tissue and less influences by CSF. As the NFBS dataset is freely accessible by members of the neuroimaging community, these possibilities may be investigated by the interested researcher.
%Iglesias et al published Dice on IBSR as 0.938 $\pm$ 0.029 for BET, 0.945 $\pm$ 0.06 for 3dSkullStrip, and 0.879 $\pm$ 0.018 for FreeSurfer HWA
    
\section*{Discussion}
In summary, we have created and shared the NFBS repository of high quality, skull-stripped T1-weighted anatomical images that is notable for its quality, its ease of access, and its heterogeneity. The procedure used to populate the repository combined the automated, state-of-the-art BEaST algorithm with  meticulous hand editing to correct any residual brain extraction errors noticed on visual inspection. The images may be used as gold standard data for comparing alternative brain extraction algorithms, as the NFBS Dice coefficients are comparable to other gold standard datasets, specifically IBSR and LPBA40 [see Figures \ref{fig:boxplot_NFBS}--\ref{fig:boxplot_I&L}]. In addition, when used as the library of segmented priors needed for using the BEaST method, NFBS may increase performance cover results using the default beast-library-1.1. 

In addition to its gold standard quality, the repository is completely open to the neuroscience community. NFBS contains no sensitive personal health information, so researchers interested in using it may do so without submitting an application or signing a data usage agreement. This is in contrast to datasets such as the one collected by the Alzheimer's Disease Neuroimaging Inititative (ADNI) \cite{pmid17476317}. Researchers can use ADNI to develop and test skullstripping algorithms \cite{pmid21195780}, but in order to do so must first apply and sign a data usage agreement due to its inclusion of sensitive information. Thus, we feel that NFBS has the potential to accelerate the pace of discovery in the field, a view that resonantes with perspectives on the importance of making neuroimaging repositories easy to access and easy to use \cite{Nichols054262}.

%With the increasing awareness in the neuroimaging community of the importance of ease-of-access to data sharing and open science initiatives such as the Enhanced Rockland Sample. NFBS thus has the potential to accelerate the pace of discovery in the field \cite{Nichols054262}.

Finally, the NFBS repository is larger and more heterogenesous than many comparable datasets. It contains 125 skull-stripped images, is composed of images from individuals with ages ranging from 21--45, and represents individuals diagnosed with a wide range of psychatric disorders [see Table \ref{psych}]. This variation is a crucial feature of NFBS, as it accounts for more than the average brain. Ultimately, this variation may prove useful for researchers interested in developing and evaluating predictive machine learning algorithms on both normal populations and those with brain disorders since data heterogeneity is an incredibly useful feature for data-driven discovery of biomarkers associated with variable clinical outcomes \cite{gabrieli2015prediction}.

\section*{Availability of supporting data}
The NFBS repository, along with Bash and Python scripts used for this paper, are available on GitHub as a part of the Preprocessed Connectomes Project \cite{Github} at {\tt https://github.com/preprocessed-connectomes-project/NFB\_skullstripped}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Abbreviations}
  MRI: magnetic resonance imaging; NFBS: Neurofeedback Skull-stripped; CSF: cerebrospinal fluid; BEaST: brain extraction based on nonlocal segmentation technique; BET: brain extraction technique; HWA: hybrid watershed technique; IBSR: Internet brain segmentation repository; LPBA40: LONI Probabilistic Brain Atlas; ADNI: Alzheimer's Disease Neuroimaging Initiative

\section*{Competing interests}
  The authors declare that they have no competing interests.

\section*{Author's contributions}
    RCC designed the Neurofeedback study and Skull-stripped repository;  BP and EST performed manual correction and validation of results; BP performed the validation analyses; BP, RCC, JSP, and JPP wrote the data note. All authors read and approved of the final version.

\section*{Acknowledgements}
  We would like to thank Dr. Simon Fristed Eskildsen for help with the installation and optimization of the BEaST method. We would also like to acknowledge Qingyang Li for creating the BEaST guide, as well as the Bash script that we based our script on. Lastly, we would like to thank all of those involved in the participation, data collection, and data sharing initiative of the Enhanced Rockland Sample.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file
\bibliography{bpuccio_nfbs_article.bib}      % Bibliography file (usually '*.bib' )
% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

%\section*{Figures}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
%\section*{Tables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Additional Files}
%  \subsection*{Additional file 1 --- Sample additional file title}
%    Additional file descriptions text (including details of how to
%    view the file, if it is in a non-standard format or the file extension).  This might
%    refer to a multi-page table or a figure.

%  \subsection*{Additional file 2 --- Sample additional file title}
%    Additional file descriptions text.




\end{backmatter}
\end{document}
